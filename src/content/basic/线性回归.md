# 线性回归模型详解

## 一、理论基础

### 1. 基本概念
线性回归是统计学和机器学习中最基础也最重要的模型之一。它通过建立因变量与自变量之间的线性关系，来预测或解释现实世界中的各种现象。

线性回归的核心思想是：假设存在一组线性关系，使得因变量可以被自变量的线性组合来表示。这种线性关系不仅简单直观，而且在许多实际应用中都表现出良好的预测能力。

### 2. 数学原理
线性回归的基本形式是：
$$
y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε
$$

这个公式中：
- y 是我们要预测的目标变量（因变量）
- x₁, x₂, ..., xₙ 是特征变量（自变量）
- β₀ 是截距项，表示当所有特征变量为0时的预测值
- β₁, β₂, ..., βₙ 是回归系数，表示每个特征变量对目标变量的影响程度
- ε 是随机误差项，表示模型无法解释的部分

### 3. 基本假设
线性回归模型建立在以下几个重要假设之上：

1. **线性性假设**
   - 自变量和因变量之间存在线性关系
   - 这种线性关系应该是加性的
   - 违反此假设会导致模型预测偏差

2. **独立性假设**
   - 观测值之间相互独立
   - 不存在自相关性
   - 常见于时间序列数据中的问题

3. **同方差性假设**
   - 误差项具有相同的方差
   - 也称为等方差性
   - 违反此假设会影响参数估计的效率

4. **正态性假设**
   - 误差项服从正态分布
   - 均值为0，方差为常数
   - 影响统计推断的有效性

5. **无多重共线性假设**
   - 自变量之间不存在完全线性相关
   - 影响参数估计的稳定性
   - 可通过VIF（方差膨胀因子）检验

## 二、数学模型

### 1. 最小二乘估计原理

最小二乘法（Ordinary Least Squares, OLS）是线性回归中最常用的参数估计方法。其核心思想是最小化预测值与实际值之间的平方误差和。

#### 1.1 目标函数
```math
min J(β) = ∑(yᵢ - ŷᵢ)² = ∑(yᵢ - (β₀ + β₁x₁ᵢ + ... + βₙxₙᵢ))²
```

这个优化问题的目标是找到一组参数β，使得所有样本点的预测误差平方和最小。

#### 1.2 求解过程
1. **求导**：对每个参数βⱼ求偏导数
2. **导数置零**：将偏导数方程组置零
3. **解方程组**：得到参数的最优解

### 2. 矩阵表示形式

#### 2.1 基本表达式
```math
y = Xβ + ε
```

其中：
- y 是n×1的响应向量
- X 是n×(p+1)的设计矩阵
- β 是(p+1)×1的参数向量
- ε 是n×1的误差向量

#### 2.2 参数估计
最小二乘估计的矩阵形式解为：
```math
β̂ = (X'X)⁻¹X'y
```

这个解具有以下特点：
- 是无偏估计
- 在高斯-马尔可夫定理下是最佳线性无偏估计（BLUE）
- 计算简单，有封闭解

### 3. 统计性质

#### 3.1 参数估计量的性质
1. **无偏性**：E(β̂) = β
2. **方差**：Var(β̂) = σ²(X'X)⁻¹
3. **一致性**：当样本量趋于无穷时，β̂趋于β

#### 3.2 预测值的性质
1. **预测值**：ŷ = Xβ̂
2. **预测方差**：Var(ŷ) = σ²X(X'X)⁻¹X'
3. **预测区间**：可构建预测值的置信区间

### 4. 模型评估指标

#### 4.1 拟合优度
1. **R²统计量**
```math
R² = 1 - SSE/SST = 1 - ∑(yᵢ - ŷᵢ)²/∑(yᵢ - ȳ)²
```

2. **调整R²**
```math
R²_adj = 1 - (1-R²)(n-1)/(n-p-1)
```

#### 4.2 假设检验
1. **F检验**：检验模型的整体显著性
2. **t检验**：检验单个参数的显著性
3. **残差分析**：检验模型假设的合理性
## 三、算法流程

### 1. 数据预处理流程

#### 1.1 数据清洗
1. **缺失值处理**
   - 删除法：直接删除含缺失值的样本
   - 填充法：均值/中位数/众数填充
   - 高级方法：基于模型的填充（如KNN填充）
   ```python
   # 示例代码
   from sklearn.impute import SimpleImputer
   imputer = SimpleImputer(strategy='mean')
   X_cleaned = imputer.fit_transform(X)
   ```

2. **异常值检测**
   - 统计方法：3σ原则、箱线图法
   - 距离方法：LOF、Isolation Forest
   - 模型方法：One-Class SVM
   ```python
   def detect_outliers(data, threshold=3):
       z_scores = np.abs((data - data.mean()) / data.std())
       return z_scores > threshold
   ```

#### 1.2 特征工程
1. **特征标准化**
   - Z-score标准化
   - Min-Max归一化
   - Robust标准化
   ```python
   from sklearn.preprocessing import StandardScaler, MinMaxScaler
   scaler = StandardScaler()
   X_scaled = scaler.fit_transform(X)
   ```

2. **特征选择**
   - 过滤法：方差、相关系数
   - 包装法：递归特征消除
   - 嵌入法：Lasso、Ridge
   ```python
   from sklearn.feature_selection import SelectKBest, f_regression
   selector = SelectKBest(f_regression, k=10)
   X_selected = selector.fit_transform(X, y)
   ```

### 2. 模型训练流程

#### 2.1 数据集划分
```python
def split_data(X, y, test_size=0.2, random_state=42):
    """
    划分训练集和测试集
    
    参数：
    X: 特征矩阵
    y: 目标变量
    test_size: 测试集比例
    random_state: 随机种子
    """
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    return X_train, X_test, y_train, y_test
```

#### 2.2 模型拟合
```python
def train_linear_model(X_train, y_train):
    """
    训练线性回归模型
    
    参数：
    X_train: 训练集特征
    y_train: 训练集标签
    """
    # 创建模型实例
    model = LinearRegression()
    
    # 模型拟合
    model.fit(X_train, y_train)
    
    # 获取模型参数
    coefficients = model.coef_
    intercept = model.intercept_
    
    return model, coefficients, intercept
```

### 3. 模型评估流程

#### 3.1 性能度量
```python
def evaluate_model(model, X_test, y_test):
    """
    评估模型性能
    
    参数：
    model: 训练好的模型
    X_test: 测试集特征
    y_test: 测试集标签
    """
    # 预测
    y_pred = model.predict(X_test)
    
    # 计算各种评估指标
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    # 返回评估结果
    return {
        'MSE': mse,
        'RMSE': rmse,
        'MAE': mae,
        'R2': r2
    }
```

#### 3.2 模型诊断
```python
def model_diagnostics(model, X, y):
    """
    模型诊断函数
    
    参数：
    model: 训练好的模型
    X: 特征矩阵
    y: 实际值
    """
    # 预测值
    y_pred = model.predict(X)
    
    # 残差
    residuals = y - y_pred
    
    # 标准化残差
    std_residuals = residuals / np.std(residuals)
    
    # 绘制诊断图
    plt.figure(figsize=(15, 5))
    
    # 残差图
    plt.subplot(131)
    plt.scatter(y_pred, residuals)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('预测值')
    plt.ylabel('残差')
    plt.title('残差vs预测值')
    
    # QQ图
    plt.subplot(132)
    stats.probplot(residuals, dist="norm", plot=plt)
    plt.title('残差QQ图')
    
    # 残差直方图
    plt.subplot(133)
    plt.hist(residuals, bins=30)
    plt.xlabel('残差')
    plt.ylabel('频数')
    plt.title('残差直方图')
    
    plt.tight_layout()
    plt.show()
```
## 四、参数设置

### 1. 核心参数详解

#### 1.1 基础参数
```python
class LinearRegression:
    def __init__(self,
                 fit_intercept=True,    # 是否计算截距
                 normalize=False,        # 是否对特征进行标准化
                 copy_X=True,           # 是否复制X
                 n_jobs=None):          # 并行计算的作业数
```

1. **fit_intercept**
   - 功能：控制是否计算截距项β₀
   - 取值：True/False
   - 建议：
     * 当数据已经中心化时，可以设为False
     * 大多数情况下建议保持默认值True

2. **normalize**
   - 功能：是否对特征进行标准化
   - 取值：True/False
   - 注意事项：
     * 建议使用StandardScaler替代
     * 在sklearn 1.0版本后已弃用

3. **copy_X**
   - 功能：是否复制输入数据
   - 取值：True/False
   - 使用场景：
     * 内存受限时可设为False
     * 需要保持原始数据时设为True

4. **n_jobs**
   - 功能：并行计算的CPU数量
   - 取值：
     * None：使用单核
     * -1：使用所有CPU
     * 正整数：指定CPU数量

### 2. 参数调优策略

#### 2.1 数据规模相关
```python
def choose_params_by_scale(n_samples, n_features):
    """
    根据数据规模选择参数
    
    参数：
    n_samples: 样本量
    n_features: 特征数
    """
    params = {}
    
    # 根据样本量选择并行度
    if n_samples > 100000:
        params['n_jobs'] = -1
    else:
        params['n_jobs'] = None
        
    # 根据特征数决定是否复制数据
    if n_features > 1000:
        params['copy_X'] = False
    else:
        params['copy_X'] = True
        
    return params
```

#### 2.2 内存优化
```python
def optimize_memory_usage(X, memory_limit):
    """
    内存优化策略
    
    参数：
    X: 特征矩阵
    memory_limit: 内存限制（GB）
    """
    # 计算数据大小
    data_size = X.nbytes / (1024 ** 3)  # 转换为GB
    
    params = {}
    if data_size > memory_limit * 0.5:  # 如果数据超过内存限制的50%
        params['copy_X'] = False
        # 建议使用批处理或数据生成器
        print("建议使用批处理方式训练模型")
    else:
        params['copy_X'] = True
        
    return params
```

### 3. 高级参数配置

#### 3.1 交叉验证参数
```python
from sklearn.model_selection import GridSearchCV

def cv_parameter_tuning(X, y):
    """
    使用交叉验证优化参数
    """
    # 定义参数网格
    param_grid = {
        'fit_intercept': [True, False],
        'normalize': [True, False],
        'copy_X': [True, False]
    }
    
    # 创建模型
    model = LinearRegression()
    
    # 网格搜索
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=5,
        scoring='r2',
        n_jobs=-1
    )
    
    # 执行搜索
    grid_search.fit(X, y)
    
    return grid_search.best_params_
```

#### 3.2 自适应参数设置
```python
def adaptive_params(X, y):
    """
    根据数据特征自适应设置参数
    """
    params = {}
    
    # 检查特征是否已标准化
    feature_means = np.mean(X, axis=0)
    feature_stds = np.std(X, axis=0)
    
    if np.all(np.abs(feature_means) < 1e-7) and np.all(np.abs(feature_stds - 1) < 1e-7):
        params['normalize'] = False
    else:
        params['normalize'] = True
    
    # 检查是否存在共线性
    from sklearn.preprocessing import StandardScaler
    X_scaled = StandardScaler().fit_transform(X)
    correlation_matrix = np.corrcoef(X_scaled.T)
    if np.any(np.abs(correlation_matrix - np.eye(X.shape[1])) > 0.9):
        print("警告：存在强相关特征，建议进行特征选择或正则化")
    
    return params
```

### 4. 参数验证与监控

#### 4.1 参数有效性检查
```python
def validate_params(params):
    """
    验证参数有效性
    """
    # 检查参数类型
    assert isinstance(params['fit_intercept'], bool), "fit_intercept must be boolean"
    assert isinstance(params['normalize'], bool), "normalize must be boolean"
    assert isinstance(params['copy_X'], bool), "copy_X must be boolean"
    
    # 检查n_jobs参数
    if params.get('n_jobs') is not None:
        assert isinstance(params['n_jobs'], int), "n_jobs must be integer"
        if params['n_jobs'] < -1:
            raise ValueError("n_jobs must be >= -1")
            
    return True
```
## 五、代码实现

### 1. 完整的线性回归实现类

```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

class LinearRegressionImplementation:
    """
    线性回归完整实现类
    """
    def __init__(self, fit_intercept=True, normalize=False):
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.coef_ = None
        self.intercept_ = None
        self.scaler = StandardScaler() if normalize else None
        
    def fit(self, X, y):
        """
        模型训练
        
        参数：
        X: 特征矩阵
        y: 目标变量
        """
        # 数据预处理
        X = self._preprocess_X(X)
        
        # 添加截距项
        if self.fit_intercept:
            X = np.column_stack([np.ones(X.shape[0]), X])
        
        # 使用最小二乘法计算参数
        beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)
        
        # 提取参数
        if self.fit_intercept:
            self.intercept_ = beta[0]
            self.coef_ = beta[1:]
        else:
            self.intercept_ = 0
            self.coef_ = beta
            
        return self
    
    def predict(self, X):
        """
        模型预测
        
        参数：
        X: 特征矩阵
        """
        # 数据预处理
        X = self._preprocess_X(X)
        
        # 预测
        return X.dot(self.coef_) + self.intercept_
    
    def _preprocess_X(self, X):
        """
        特征预处理
        """
        X = np.asarray(X)
        if self.normalize:
            if len(X.shape) == 1:
                X = X.reshape(-1, 1)
            return self.scaler.fit_transform(X)
        return X
    
    def score(self, X, y):
        """
        计算R²分数
        """
        return r2_score(y, self.predict(X))
    
    def get_params(self):
        """
        获取模型参数
        """
        return {
            'coefficients': self.coef_,
            'intercept': self.intercept_
        }

```

### 2. 模型训练与评估工具类

```python
class ModelEvaluator:
    """
    模型评估工具类
    """
    def __init__(self, model):
        self.model = model
        self.metrics = {}
        
    def evaluate(self, X_test, y_test):
        """
        评估模型性能
        """
        y_pred = self.model.predict(X_test)
        
        # 计算各种评估指标
        self.metrics['r2'] = r2_score(y_test, y_pred)
        self.metrics['mse'] = mean_squared_error(y_test, y_pred)
        self.metrics['rmse'] = np.sqrt(self.metrics['mse'])
        self.metrics['mae'] = mean_absolute_error(y_test, y_pred)
        
        return self.metrics
    
    def plot_diagnostics(self, X_test, y_test):
        """
        绘制诊断图
        """
        y_pred = self.model.predict(X_test)
        residuals = y_test - y_pred
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 残差vs预测值
        axes[0,0].scatter(y_pred, residuals)
        axes[0,0].axhline(y=0, color='r', linestyle='--')
        axes[0,0].set_xlabel('预测值')
        axes[0,0].set_ylabel('残差')
        axes[0,0].set_title('残差 vs 预测值')
        
        # 残差直方图
        axes[0,1].hist(residuals, bins=30)
        axes[0,1].set_xlabel('残差')
        axes[0,1].set_ylabel('频数')
        axes[0,1].set_title('残差分布')
        
        # QQ图
        stats.probplot(residuals, dist="norm", plot=axes[1,0])
        axes[1,0].set_title('残差QQ图')
        
        # 实际值vs预测值
        axes[1,1].scatter(y_test, y_pred)
        axes[1,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
        axes[1,1].set_xlabel('实际值')
        axes[1,1].set_ylabel('预测值')
        axes[1,1].set_title('实际值 vs 预测值')
        
        plt.tight_layout()
        plt.show()
```

### 3. 特征工程工具类

```python
class FeatureProcessor:
    """
    特征处理工具类
    """
    def __init__(self):
        self.scaler = StandardScaler()
        
    def process_features(self, X, categorical_cols=None, numerical_cols=None):
        """
        特征处理主函数
        """
        X = pd.DataFrame(X)
        
        if categorical_cols is None:
            categorical_cols = X.select_dtypes(include=['object']).columns
        if numerical_cols is None:
            numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns
            
        # 处理类别特征
        X_cat = self._process_categorical(X[categorical_cols])
        
        # 处理数值特征
        X_num = self._process_numerical(X[numerical_cols])
        
        # 合并特征
        X_processed = pd.concat([X_num, X_cat], axis=1)
        
        return X_processed
    
    def _process_categorical(self, X_cat):
        """
        处理类别特征
        """
        # 对类别特征进行独热编码
        return pd.get_dummies(X_cat, drop_first=True)
    
    def _process_numerical(self, X_num):
        """
        处理数值特征
        """
        # 标准化数值特征
        return pd.DataFrame(
            self.scaler.fit_transform(X_num),
            columns=X_num.columns
        )
```

### 4. 完整的建模流程示例

```python
def modeling_pipeline(X, y, test_size=0.2, random_state=42):
    """
    完整的建模流程
    """
    # 1. 数据集划分
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    # 2. 特征处理
    processor = FeatureProcessor()
    X_train_processed = processor.process_features(X_train)
    X_test_processed = processor.process_features(X_test)
    
    # 3. 模型训练
    model = LinearRegressionImplementation(fit_intercept=True, normalize=True)
    model.fit(X_train_processed, y_train)
    
    # 4. 模型评估
    evaluator = ModelEvaluator(model)
    metrics = evaluator.evaluate(X_test_processed, y_test)
    
    # 5. 模型诊断
    evaluator.plot_diagnostics(X_test_processed, y_test)
    
    return model, metrics
```

## 六、应用案例
## 六、应用案例

### 1. 简单线性回归示例

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

class SimpleRegressionExample:
    """
    简单线性回归示例
    """
    def __init__(self, n_samples=1000):
        self.n_samples = n_samples
        self.X = None
        self.y = None
        self.model = None
        
    def generate_data(self):
        """
        生成模拟数据
        """
        # 生成自变量
        np.random.seed(42)
        X1 = np.random.normal(0, 1, self.n_samples)
        X2 = np.random.normal(0, 1, self.n_samples)
        X3 = np.random.normal(0, 1, self.n_samples)
        
        # 生成因变量（带有一些非线性关系和噪声）
        y = 2*X1 - 1.5*X2 + 0.5*X3 + 0.2*X1**2 + np.random.normal(0, 0.1, self.n_samples)
        
        # 创建特征矩阵
        self.X = pd.DataFrame({
            'X1': X1,
            'X2': X2,
            'X3': X3
        })
        self.y = pd.Series(y, name='y')
        
        return self
        
    def explore_data(self):
        """
        数据探索
        """
        # 散点图矩阵
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # X1 vs y
        axes[0,0].scatter(self.X['X1'], self.y, alpha=0.5)
        axes[0,0].set_xlabel('X1')
        axes[0,0].set_ylabel('y')
        axes[0,0].set_title('X1 vs y')
        
        # X2 vs y
        axes[0,1].scatter(self.X['X2'], self.y, alpha=0.5)
        axes[0,1].set_xlabel('X2')
        axes[0,1].set_ylabel('y')
        axes[0,1].set_title('X2 vs y')
        
        # X3 vs y
        axes[1,0].scatter(self.X['X3'], self.y, alpha=0.5)
        axes[1,0].set_xlabel('X3')
        axes[1,0].set_ylabel('y')
        axes[1,0].set_title('X3 vs y')
        
        # 相关性热力图
        corr = pd.concat([self.X, self.y], axis=1).corr()
        sns.heatmap(corr, annot=True, cmap='coolwarm', ax=axes[1,1])
        axes[1,1].set_title('相关性热力图')
        
        plt.tight_layout()
        plt.show()
        
    def train_model(self):
        """
        训练模型
        """
        # 数据分割
        X_train, X_test, y_train, y_test = train_test_split(
            self.X, self.y, test_size=0.2, random_state=42
        )
        
        # 标准化
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # 训练模型
        self.model = LinearRegressionImplementation(fit_intercept=True)
        self.model.fit(X_train_scaled, y_train)
        
        # 预测
        y_pred = self.model.predict(X_test_scaled)
        
        # 计算评估指标
        metrics = {
            'R2': r2_score(y_test, y_pred),
            'MSE': mean_squared_error(y_test, y_pred),
            'MAE': mean_absolute_error(y_test, y_pred)
        }
        
        return metrics
        
    def plot_predictions(self):
        """
        绘制预测结果
        """
        # 使用全部数据进行预测
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(self.X)
        y_pred = self.model.predict(X_scaled)
        
        plt.figure(figsize=(10, 6))
        plt.scatter(self.y, y_pred, alpha=0.5)
        plt.plot([self.y.min(), self.y.max()], [self.y.min(), self.y.max()], 'r--')
        plt.xlabel('实际值')
        plt.ylabel('预测值')
        plt.title('实际值 vs 预测值')
        plt.tight_layout()
        plt.show()
        
    def feature_importance(self):
        """
        分析特征重要性
        """
        # 获取特征系数
        coefficients = pd.DataFrame({
            'Feature': self.X.columns,
            'Coefficient': self.model.coef_
        })
        
        # 绘制特征重要性条形图
        plt.figure(figsize=(8, 5))
        plt.bar(coefficients['Feature'], coefficients['Coefficient'])
        plt.title('特征重要性')
        plt.xlabel('特征')
        plt.ylabel('系数')
        plt.xticks(rotation=0)
        plt.tight_layout()
        plt.show()
        
        return coefficients

def run_simple_regression():
    """
    运行简单回归示例
    """
    # 创建实例
    example = SimpleRegressionExample(n_samples=1000)
    
    # 生成数据
    example.generate_data()
    print("数据生成完成！")
    
    # 数据探索
    print("\n开始数据探索...")
    example.explore_data()
    
    # 训练模型
    print("\n开始训练模型...")
    metrics = example.train_model()
    print("\n模型评估指标：")
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")
    
    # 绘制预测结果
    print("\n绘制预测结果...")
    example.plot_predictions()
    
    # 分析特征重要性
    print("\n分析特征重要性...")
    importance = example.feature_importance()
    print("\n特征重要性：")
    print(importance)

if __name__ == "__main__":
    run_simple_regression()
```

### 2. 多变量回归示例

```python
class MultipleRegressionExample:
    """
    多变量回归示例
    """
    def __init__(self, n_samples=1000, n_features=5):
        self.n_samples = n_samples
        self.n_features = n_features
        self.X = None
        self.y = None
        self.model = None
        
    def generate_data(self):
        """
        生成多变量数据
        """
        np.random.seed(42)
        
        # 生成特征
        X = np.random.randn(self.n_samples, self.n_features)
        
        # 生成系数
        true_coefficients = np.array([1, -0.5, 0.25, -0.1, 0.05])
        
        # 生成目标变量（加入非线性项和噪声）
        y = np.dot(X, true_coefficients) + \
            0.1 * X[:, 0]**2 + \
            0.05 * X[:, 1]**2 + \
            np.random.normal(0, 0.1, self.n_samples)
            
        # 转换为DataFrame
        self.X = pd.DataFrame(
            X, 
            columns=[f'X{i+1}' for i in range(self.n_features)]
        )
        self.y = pd.Series(y, name='y')
        
        return self
        
    def add_polynomial_features(self):
        """
        添加多项式特征
        """
        for i in range(self.n_features):
            col_name = f'X{i+1}'
            self.X[f'{col_name}_squared'] = self.X[col_name]**2
            
        return self
        
    def train_with_cross_validation(self, cv=5):
        """
        使用交叉验证训练模型
        """
        from sklearn.model_selection import KFold
        
        # 初始化K折交叉验证
        kf = KFold(n_splits=cv, shuffle=True, random_state=42)
        
        # 存储每折的评估指标
        metrics_list = []
        
        for fold, (train_idx, val_idx) in enumerate(kf.split(self.X)):
            # 分割数据
            X_train = self.X.iloc[train_idx]
            y_train = self.y.iloc[train_idx]
            X_val = self.X.iloc[val_idx]
            y_val = self.y.iloc[val_idx]
            
            # 标准化
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)
            
            # 训练模型
            model = LinearRegressionImplementation(fit_intercept=True)
            model.fit(X_train_scaled, y_train)
            
            # 预测
            y_pred = model.predict(X_val_scaled)
            
            # 计算评估指标
            fold_metrics = {
                'Fold': fold + 1,
                'R2': r2_score(y_val, y_pred),
                'MSE': mean_squared_error(y_val, y_pred),
                'MAE': mean_absolute_error(y_val, y_pred)
            }
            metrics_list.append(fold_metrics)
        
        # 计算平均指标
        avg_metrics = pd.DataFrame(metrics_list).mean()
        return avg_metrics, metrics_list

def run_multiple_regression():
    """
    运行多变量回归示例
    """
    # 创建实例
    example = MultipleRegressionExample(n_samples=1000, n_features=5)
    
    # 生成数据
    example.generate_data()
    print("原始特征数量:", example.X.shape[1])
    
    # 添加多项式特征
    example.add_polynomial_features()
    print("添加多项式特征后的特征数量:", example.X.shape[1])
    
    # 交叉验证训练
    print("\n开始交叉验证训练...")
    avg_metrics, fold_metrics = example.train_with_cross_validation()
    
    print("\n各折评估指标：")
    for metrics in fold_metrics:
        print(f"Fold {metrics['Fold']}:")
        for key, value in metrics.items():
            if key != 'Fold':
                print(f"  {key}: {value:.4f}")
                
    print("\n平均评估指标：")
    for key, value in avg_metrics.items():
        if key != 'Fold':
            print(f"{key}: {value:.4f}")

if __name__ == "__main__":
    print("运行简单回归示例：")
    run_simple_regression()
    print("\n" + "="*50 + "\n")
    print("运行多变量回归示例：")
    run_multiple_regression()
```

结果如下：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/50dcd7e34aca4ad089d276859a5d73a0.png#pic_center)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a8cfcaa4c679404daf40e9f6d1d5ef5f.png#pic_center)
# 线性回归模型完整总结

## 一、核心要点回顾

### 1. 理论基础
- 线性回归是最基础的监督学习算法
- 基于最小二乘法原理进行参数估计
- 需要满足线性性、独立性、同方差性等基本假设

### 2. 数学模型
- 基本形式：y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε
- 矩阵形式：y = Xβ + ε
- 参数估计：β = (X'X)⁻¹X'y

### 3. 实现流程
1. 数据预处理
   - 缺失值处理
   - 异常值检测
   - 特征工程
   - 数据标准化

2. 模型训练
   - 数据集划分
   - 参数估计
   - 模型拟合

3. 模型评估
   - R²评分
   - MSE/RMSE
   - MAE
   - 残差分析

## 二、关键优势

1. **简单直观**
   - 模型结构清晰
   - 计算效率高
   - 易于实现和部署

2. **可解释性强**
   - 系数直观反映特征重要性
   - 便于业务理解和决策

3. **适用性广**
   - 可处理多种类型的回归问题
   - 可作为基准模型

## 三、主要局限

1. **模型假设严格**
   - 需要满足线性假设
   - 对异常值敏感
   - 特征间不能存在多重共线性

2. **预测能力有限**
   - 无法处理非线性关系
   - 难以捕捉复杂模式
   - 容易欠拟合

## 四、实践建议

1. **数据预处理**
   - 重视特征工程
   - 注意数据标准化
   - 谨慎处理异常值

2. **模型选择**
   - 先尝试基本线性模型
   - 根据需要添加正则化
   - 考虑多项式特征

3. **模型诊断**
   - 进行充分的残差分析
   - 检查模型假设
   - 注意过拟合问题

## 五、发展方向

1. **模型改进**
   - 引入正则化（Ridge, Lasso, Elastic Net）
   - 考虑非线性变换
   - 集成学习方法

2. **应用拓展**
   - 结合深度学习
   - 时间序列分析
   - 因果推断

3. **工程实践**
   - 模型部署优化
   - 在线学习方案
   - 自动化建模流程

## 六、总体评价

线性回归作为机器学习的基础模型，虽然简单，但在实际应用中仍具有重要价值。它不仅提供了解决回归问题的基本思路，还为更复杂的模型提供了重要的参考基准。在实践中，应该充分认识其优势和局限，合理选择使用场景，必要时考虑模型改进或组合使用其他方法。